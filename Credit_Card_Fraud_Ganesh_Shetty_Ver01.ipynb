{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credit Card Fraud Prediction\n",
    "\n",
    "This Credit card fraud prediction model is built on the data available in Kaggle. It is important that credit card companies are able to recognize fraudulent credit card transactions so that customers are not charged for items that they did not purchase.\n",
    "\n",
    "**Iroduction:**\n",
    "\n",
    "The datasets contains transactions made by credit cards in September 2013 by european cardholders.\n",
    "This dataset presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions. The dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions.\n",
    "\n",
    "It contains only numerical input variables which are the result of a PCA transformation. Unfortunately, due to confidentiality issues, we cannot provide the original features and more background information about the data. Features V1, V2, … V28 are the principal components obtained with PCA, the only features which have not been transformed with PCA are 'Time' and 'Amount'. Feature 'Time' contains the seconds elapsed between each transaction and the first transaction in the dataset. The feature 'Amount' is the transaction Amount, this feature can be used for example-dependant cost-senstive learning. Feature 'Class' is the response variable and it takes value 1 in case of fraud and 0 otherwise.\n",
    "\n",
    "**Work So far**\n",
    "\n",
    "In this kernel I tried to predict credit card fraud by using Logistic Regression and Random Forest. Since the data is highly imbalanced I tried different sampling methods. There is still some scope of improvement left. I will concentrate on this part in the near future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing all important Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Dataset\n",
    "df=pd.read_csv('creditcard.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.359807</td>\n",
       "      <td>-0.072781</td>\n",
       "      <td>2.536347</td>\n",
       "      <td>1.378155</td>\n",
       "      <td>-0.338321</td>\n",
       "      <td>0.462388</td>\n",
       "      <td>0.239599</td>\n",
       "      <td>0.098698</td>\n",
       "      <td>0.363787</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018307</td>\n",
       "      <td>0.277838</td>\n",
       "      <td>-0.110474</td>\n",
       "      <td>0.066928</td>\n",
       "      <td>0.128539</td>\n",
       "      <td>-0.189115</td>\n",
       "      <td>0.133558</td>\n",
       "      <td>-0.021053</td>\n",
       "      <td>149.62</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.191857</td>\n",
       "      <td>0.266151</td>\n",
       "      <td>0.166480</td>\n",
       "      <td>0.448154</td>\n",
       "      <td>0.060018</td>\n",
       "      <td>-0.082361</td>\n",
       "      <td>-0.078803</td>\n",
       "      <td>0.085102</td>\n",
       "      <td>-0.255425</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.225775</td>\n",
       "      <td>-0.638672</td>\n",
       "      <td>0.101288</td>\n",
       "      <td>-0.339846</td>\n",
       "      <td>0.167170</td>\n",
       "      <td>0.125895</td>\n",
       "      <td>-0.008983</td>\n",
       "      <td>0.014724</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.358354</td>\n",
       "      <td>-1.340163</td>\n",
       "      <td>1.773209</td>\n",
       "      <td>0.379780</td>\n",
       "      <td>-0.503198</td>\n",
       "      <td>1.800499</td>\n",
       "      <td>0.791461</td>\n",
       "      <td>0.247676</td>\n",
       "      <td>-1.514654</td>\n",
       "      <td>...</td>\n",
       "      <td>0.247998</td>\n",
       "      <td>0.771679</td>\n",
       "      <td>0.909412</td>\n",
       "      <td>-0.689281</td>\n",
       "      <td>-0.327642</td>\n",
       "      <td>-0.139097</td>\n",
       "      <td>-0.055353</td>\n",
       "      <td>-0.059752</td>\n",
       "      <td>378.66</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.966272</td>\n",
       "      <td>-0.185226</td>\n",
       "      <td>1.792993</td>\n",
       "      <td>-0.863291</td>\n",
       "      <td>-0.010309</td>\n",
       "      <td>1.247203</td>\n",
       "      <td>0.237609</td>\n",
       "      <td>0.377436</td>\n",
       "      <td>-1.387024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.108300</td>\n",
       "      <td>0.005274</td>\n",
       "      <td>-0.190321</td>\n",
       "      <td>-1.175575</td>\n",
       "      <td>0.647376</td>\n",
       "      <td>-0.221929</td>\n",
       "      <td>0.062723</td>\n",
       "      <td>0.061458</td>\n",
       "      <td>123.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>-1.158233</td>\n",
       "      <td>0.877737</td>\n",
       "      <td>1.548718</td>\n",
       "      <td>0.403034</td>\n",
       "      <td>-0.407193</td>\n",
       "      <td>0.095921</td>\n",
       "      <td>0.592941</td>\n",
       "      <td>-0.270533</td>\n",
       "      <td>0.817739</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009431</td>\n",
       "      <td>0.798278</td>\n",
       "      <td>-0.137458</td>\n",
       "      <td>0.141267</td>\n",
       "      <td>-0.206010</td>\n",
       "      <td>0.502292</td>\n",
       "      <td>0.219422</td>\n",
       "      <td>0.215153</td>\n",
       "      <td>69.99</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
       "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
       "2   1.0 -1.358354 -1.340163  1.773209  0.379780 -0.503198  1.800499  0.791461   \n",
       "3   1.0 -0.966272 -0.185226  1.792993 -0.863291 -0.010309  1.247203  0.237609   \n",
       "4   2.0 -1.158233  0.877737  1.548718  0.403034 -0.407193  0.095921  0.592941   \n",
       "\n",
       "         V8        V9  ...       V21       V22       V23       V24       V25  \\\n",
       "0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539   \n",
       "1  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170   \n",
       "2  0.247676 -1.514654  ...  0.247998  0.771679  0.909412 -0.689281 -0.327642   \n",
       "3  0.377436 -1.387024  ... -0.108300  0.005274 -0.190321 -1.175575  0.647376   \n",
       "4 -0.270533  0.817739  ... -0.009431  0.798278 -0.137458  0.141267 -0.206010   \n",
       "\n",
       "        V26       V27       V28  Amount  Class  \n",
       "0 -0.189115  0.133558 -0.021053  149.62      0  \n",
       "1  0.125895 -0.008983  0.014724    2.69      0  \n",
       "2 -0.139097 -0.055353 -0.059752  378.66      0  \n",
       "3 -0.221929  0.062723  0.061458  123.50      0  \n",
       "4  0.502292  0.219422  0.215153   69.99      0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Brief Overview of the Data\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations**\n",
    "- We can see that variables V1 to V28 are scaled by dataset providers along with that the names of these variables are not shared to maintain secrecy.\n",
    "- However Variables Time and Amount are not scaled\n",
    "- We have target Classes 0 (Non-Fraud) and 1(Fraud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 284807 entries, 0 to 284806\n",
      "Data columns (total 31 columns):\n",
      " #   Column  Non-Null Count   Dtype  \n",
      "---  ------  --------------   -----  \n",
      " 0   Time    284807 non-null  float64\n",
      " 1   V1      284807 non-null  float64\n",
      " 2   V2      284807 non-null  float64\n",
      " 3   V3      284807 non-null  float64\n",
      " 4   V4      284807 non-null  float64\n",
      " 5   V5      284807 non-null  float64\n",
      " 6   V6      284807 non-null  float64\n",
      " 7   V7      284807 non-null  float64\n",
      " 8   V8      284807 non-null  float64\n",
      " 9   V9      284807 non-null  float64\n",
      " 10  V10     284807 non-null  float64\n",
      " 11  V11     284807 non-null  float64\n",
      " 12  V12     284807 non-null  float64\n",
      " 13  V13     284807 non-null  float64\n",
      " 14  V14     284807 non-null  float64\n",
      " 15  V15     284807 non-null  float64\n",
      " 16  V16     284807 non-null  float64\n",
      " 17  V17     284807 non-null  float64\n",
      " 18  V18     284807 non-null  float64\n",
      " 19  V19     284807 non-null  float64\n",
      " 20  V20     284807 non-null  float64\n",
      " 21  V21     284807 non-null  float64\n",
      " 22  V22     284807 non-null  float64\n",
      " 23  V23     284807 non-null  float64\n",
      " 24  V24     284807 non-null  float64\n",
      " 25  V25     284807 non-null  float64\n",
      " 26  V26     284807 non-null  float64\n",
      " 27  V27     284807 non-null  float64\n",
      " 28  V28     284807 non-null  float64\n",
      " 29  Amount  284807 non-null  float64\n",
      " 30  Class   284807 non-null  int64  \n",
      "dtypes: float64(30), int64(1)\n",
      "memory usage: 67.4 MB\n"
     ]
    }
   ],
   "source": [
    "# Information about the dataset\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation**\n",
    "- There are no null values and no missing values in the dataset\n",
    "- Datatypes of the variables are also clean\n",
    "- not much of data cleaning is required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Time</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V21</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Amount</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>284807.000000</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>...</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>2.848070e+05</td>\n",
       "      <td>284807.000000</td>\n",
       "      <td>284807.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>94813.859575</td>\n",
       "      <td>3.919560e-15</td>\n",
       "      <td>5.688174e-16</td>\n",
       "      <td>-8.769071e-15</td>\n",
       "      <td>2.782312e-15</td>\n",
       "      <td>-1.552563e-15</td>\n",
       "      <td>2.010663e-15</td>\n",
       "      <td>-1.694249e-15</td>\n",
       "      <td>-1.927028e-16</td>\n",
       "      <td>-3.137024e-15</td>\n",
       "      <td>...</td>\n",
       "      <td>1.537294e-16</td>\n",
       "      <td>7.959909e-16</td>\n",
       "      <td>5.367590e-16</td>\n",
       "      <td>4.458112e-15</td>\n",
       "      <td>1.453003e-15</td>\n",
       "      <td>1.699104e-15</td>\n",
       "      <td>-3.660161e-16</td>\n",
       "      <td>-1.206049e-16</td>\n",
       "      <td>88.349619</td>\n",
       "      <td>0.001727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>47488.145955</td>\n",
       "      <td>1.958696e+00</td>\n",
       "      <td>1.651309e+00</td>\n",
       "      <td>1.516255e+00</td>\n",
       "      <td>1.415869e+00</td>\n",
       "      <td>1.380247e+00</td>\n",
       "      <td>1.332271e+00</td>\n",
       "      <td>1.237094e+00</td>\n",
       "      <td>1.194353e+00</td>\n",
       "      <td>1.098632e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>7.345240e-01</td>\n",
       "      <td>7.257016e-01</td>\n",
       "      <td>6.244603e-01</td>\n",
       "      <td>6.056471e-01</td>\n",
       "      <td>5.212781e-01</td>\n",
       "      <td>4.822270e-01</td>\n",
       "      <td>4.036325e-01</td>\n",
       "      <td>3.300833e-01</td>\n",
       "      <td>250.120109</td>\n",
       "      <td>0.041527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.640751e+01</td>\n",
       "      <td>-7.271573e+01</td>\n",
       "      <td>-4.832559e+01</td>\n",
       "      <td>-5.683171e+00</td>\n",
       "      <td>-1.137433e+02</td>\n",
       "      <td>-2.616051e+01</td>\n",
       "      <td>-4.355724e+01</td>\n",
       "      <td>-7.321672e+01</td>\n",
       "      <td>-1.343407e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.483038e+01</td>\n",
       "      <td>-1.093314e+01</td>\n",
       "      <td>-4.480774e+01</td>\n",
       "      <td>-2.836627e+00</td>\n",
       "      <td>-1.029540e+01</td>\n",
       "      <td>-2.604551e+00</td>\n",
       "      <td>-2.256568e+01</td>\n",
       "      <td>-1.543008e+01</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>54201.500000</td>\n",
       "      <td>-9.203734e-01</td>\n",
       "      <td>-5.985499e-01</td>\n",
       "      <td>-8.903648e-01</td>\n",
       "      <td>-8.486401e-01</td>\n",
       "      <td>-6.915971e-01</td>\n",
       "      <td>-7.682956e-01</td>\n",
       "      <td>-5.540759e-01</td>\n",
       "      <td>-2.086297e-01</td>\n",
       "      <td>-6.430976e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.283949e-01</td>\n",
       "      <td>-5.423504e-01</td>\n",
       "      <td>-1.618463e-01</td>\n",
       "      <td>-3.545861e-01</td>\n",
       "      <td>-3.171451e-01</td>\n",
       "      <td>-3.269839e-01</td>\n",
       "      <td>-7.083953e-02</td>\n",
       "      <td>-5.295979e-02</td>\n",
       "      <td>5.600000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>84692.000000</td>\n",
       "      <td>1.810880e-02</td>\n",
       "      <td>6.548556e-02</td>\n",
       "      <td>1.798463e-01</td>\n",
       "      <td>-1.984653e-02</td>\n",
       "      <td>-5.433583e-02</td>\n",
       "      <td>-2.741871e-01</td>\n",
       "      <td>4.010308e-02</td>\n",
       "      <td>2.235804e-02</td>\n",
       "      <td>-5.142873e-02</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.945017e-02</td>\n",
       "      <td>6.781943e-03</td>\n",
       "      <td>-1.119293e-02</td>\n",
       "      <td>4.097606e-02</td>\n",
       "      <td>1.659350e-02</td>\n",
       "      <td>-5.213911e-02</td>\n",
       "      <td>1.342146e-03</td>\n",
       "      <td>1.124383e-02</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>139320.500000</td>\n",
       "      <td>1.315642e+00</td>\n",
       "      <td>8.037239e-01</td>\n",
       "      <td>1.027196e+00</td>\n",
       "      <td>7.433413e-01</td>\n",
       "      <td>6.119264e-01</td>\n",
       "      <td>3.985649e-01</td>\n",
       "      <td>5.704361e-01</td>\n",
       "      <td>3.273459e-01</td>\n",
       "      <td>5.971390e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>1.863772e-01</td>\n",
       "      <td>5.285536e-01</td>\n",
       "      <td>1.476421e-01</td>\n",
       "      <td>4.395266e-01</td>\n",
       "      <td>3.507156e-01</td>\n",
       "      <td>2.409522e-01</td>\n",
       "      <td>9.104512e-02</td>\n",
       "      <td>7.827995e-02</td>\n",
       "      <td>77.165000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>172792.000000</td>\n",
       "      <td>2.454930e+00</td>\n",
       "      <td>2.205773e+01</td>\n",
       "      <td>9.382558e+00</td>\n",
       "      <td>1.687534e+01</td>\n",
       "      <td>3.480167e+01</td>\n",
       "      <td>7.330163e+01</td>\n",
       "      <td>1.205895e+02</td>\n",
       "      <td>2.000721e+01</td>\n",
       "      <td>1.559499e+01</td>\n",
       "      <td>...</td>\n",
       "      <td>2.720284e+01</td>\n",
       "      <td>1.050309e+01</td>\n",
       "      <td>2.252841e+01</td>\n",
       "      <td>4.584549e+00</td>\n",
       "      <td>7.519589e+00</td>\n",
       "      <td>3.517346e+00</td>\n",
       "      <td>3.161220e+01</td>\n",
       "      <td>3.384781e+01</td>\n",
       "      <td>25691.160000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                Time            V1            V2            V3            V4  \\\n",
       "count  284807.000000  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
       "mean    94813.859575  3.919560e-15  5.688174e-16 -8.769071e-15  2.782312e-15   \n",
       "std     47488.145955  1.958696e+00  1.651309e+00  1.516255e+00  1.415869e+00   \n",
       "min         0.000000 -5.640751e+01 -7.271573e+01 -4.832559e+01 -5.683171e+00   \n",
       "25%     54201.500000 -9.203734e-01 -5.985499e-01 -8.903648e-01 -8.486401e-01   \n",
       "50%     84692.000000  1.810880e-02  6.548556e-02  1.798463e-01 -1.984653e-02   \n",
       "75%    139320.500000  1.315642e+00  8.037239e-01  1.027196e+00  7.433413e-01   \n",
       "max    172792.000000  2.454930e+00  2.205773e+01  9.382558e+00  1.687534e+01   \n",
       "\n",
       "                 V5            V6            V7            V8            V9  \\\n",
       "count  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
       "mean  -1.552563e-15  2.010663e-15 -1.694249e-15 -1.927028e-16 -3.137024e-15   \n",
       "std    1.380247e+00  1.332271e+00  1.237094e+00  1.194353e+00  1.098632e+00   \n",
       "min   -1.137433e+02 -2.616051e+01 -4.355724e+01 -7.321672e+01 -1.343407e+01   \n",
       "25%   -6.915971e-01 -7.682956e-01 -5.540759e-01 -2.086297e-01 -6.430976e-01   \n",
       "50%   -5.433583e-02 -2.741871e-01  4.010308e-02  2.235804e-02 -5.142873e-02   \n",
       "75%    6.119264e-01  3.985649e-01  5.704361e-01  3.273459e-01  5.971390e-01   \n",
       "max    3.480167e+01  7.330163e+01  1.205895e+02  2.000721e+01  1.559499e+01   \n",
       "\n",
       "       ...           V21           V22           V23           V24  \\\n",
       "count  ...  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05   \n",
       "mean   ...  1.537294e-16  7.959909e-16  5.367590e-16  4.458112e-15   \n",
       "std    ...  7.345240e-01  7.257016e-01  6.244603e-01  6.056471e-01   \n",
       "min    ... -3.483038e+01 -1.093314e+01 -4.480774e+01 -2.836627e+00   \n",
       "25%    ... -2.283949e-01 -5.423504e-01 -1.618463e-01 -3.545861e-01   \n",
       "50%    ... -2.945017e-02  6.781943e-03 -1.119293e-02  4.097606e-02   \n",
       "75%    ...  1.863772e-01  5.285536e-01  1.476421e-01  4.395266e-01   \n",
       "max    ...  2.720284e+01  1.050309e+01  2.252841e+01  4.584549e+00   \n",
       "\n",
       "                V25           V26           V27           V28         Amount  \\\n",
       "count  2.848070e+05  2.848070e+05  2.848070e+05  2.848070e+05  284807.000000   \n",
       "mean   1.453003e-15  1.699104e-15 -3.660161e-16 -1.206049e-16      88.349619   \n",
       "std    5.212781e-01  4.822270e-01  4.036325e-01  3.300833e-01     250.120109   \n",
       "min   -1.029540e+01 -2.604551e+00 -2.256568e+01 -1.543008e+01       0.000000   \n",
       "25%   -3.171451e-01 -3.269839e-01 -7.083953e-02 -5.295979e-02       5.600000   \n",
       "50%    1.659350e-02 -5.213911e-02  1.342146e-03  1.124383e-02      22.000000   \n",
       "75%    3.507156e-01  2.409522e-01  9.104512e-02  7.827995e-02      77.165000   \n",
       "max    7.519589e+00  3.517346e+00  3.161220e+01  3.384781e+01   25691.160000   \n",
       "\n",
       "               Class  \n",
       "count  284807.000000  \n",
       "mean        0.001727  \n",
       "std         0.041527  \n",
       "min         0.000000  \n",
       "25%         0.000000  \n",
       "50%         0.000000  \n",
       "75%         0.000000  \n",
       "max         1.000000  \n",
       "\n",
       "[8 rows x 31 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Statistical summary of data\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to scale the Time and Amount Variables. Except these all other variables are already scaled\n",
    "\n",
    "Information from dataset providers:\n",
    "<br>\"It contains only numerical input variables which are the result of a PCA transformation. Unfortunately, due to confidentiality issues, we cannot provide the original features and more background information about the data. Features V1, V2, … V28 are the principal components obtained with PCA, the only features which have not been transformed with PCA are 'Time' and 'Amount'. Feature 'Time' contains the seconds elapsed between each transaction and the first transaction in the dataset. The feature 'Amount' is the transaction Amount, this feature can be used for example-dependant cost-senstive learning. Feature 'Class' is the response variable and it takes value 1 in case of fraud and 0 otherwise.\"\n",
    "\n",
    "important piece of information\n",
    "<br> https://datascience.stackexchange.com/questions/54908/data-normalization-before-or-after-train-test-split\n",
    "\n",
    "Ideally we should be scaling after the train test split. However, Since other variables are already scaled for PCA. It makes sense scaling Time and Amount features here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling of Time and Amount variables\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "rob = RobustScaler()\n",
    "\n",
    "df['amount'] = rob.fit_transform(df['Amount'].values.reshape(-1,1))\n",
    "df['time'] = rob.fit_transform(df['Time'].values.reshape(-1,1))\n",
    "\n",
    "\n",
    "# Lets drop the Time and Amount columns\n",
    "df.drop(['Time','Amount'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Checking the Balance of the Target Class\n",
    "pie, ax = plt.subplots(figsize=[10,6])\n",
    "labels = ['Non-Fraud','Fraud']\n",
    "plt.pie(x=round(df['Class'].value_counts()/len(df) * 100,2), labels=labels,autopct=\"%.1f%%\", pctdistance=0.5)\n",
    "plt.title(\"Distribution of Target Variable\", fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    284315\n",
       "1       492\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Class.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can see that Target class is highly imbalanced where 99.8% of the cases are non fraud cases. \n",
    "<br>Such an imbalance in data can lead to overfitting. Our model will be biased towards non fraud cases\n",
    "<br>Let us check following options:\n",
    "\n",
    "(1) Without resampling\n",
    "\n",
    "(2) Under Sampling\n",
    "\n",
    "(3) Over Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Without Resampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'C': 1.0}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "X = df.drop('Class',axis=1)\n",
    "y = df['Class']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=101)\n",
    "\n",
    "parameters = {\n",
    "    'C': np.linspace(1, 10, 10)\n",
    "             }\n",
    "logmodel = LogisticRegression()\n",
    "clf = GridSearchCV(logmodel, parameters, cv=5, verbose=5, n_jobs=3)\n",
    "clf.fit(X_train, y_train.ravel())\n",
    "\n",
    "clf.best_params_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    4.0s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    4.0s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1, verbose=5)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logmodel1 = LogisticRegression(C=1,penalty='l2', verbose=5)\n",
    "logmodel1.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predictions and Evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     56859\n",
      "           1       0.91      0.61      0.73       103\n",
      "\n",
      "    accuracy                           1.00     56962\n",
      "   macro avg       0.96      0.81      0.87     56962\n",
      "weighted avg       1.00      1.00      1.00     56962\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = logmodel1.predict(X_test)\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observation:**\n",
    "Fraud cases have a recall of 61% that means of all the Fraud cases only 61% are correctly predicted.\n",
    "\n",
    "Lets see if we can better this with Under Sampling. However based on the information below we will have to device a strategy for under sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downsampling generally tends to result in overfitting within whatever model we create. For example if most fraudsters commit fraud on amount >€3000, then our Under-sampled data shows all frauds with >€3000 and the Non-Frauds as a random sample of the huge Non-Fraud population, where overwhelming majority of transactions might be ~ €100. So eventually we will have a dataset with all 1s having avg Amount €3000 and all 0s having avg amount ~€100. Hence, very highly separable!\n",
    "\n",
    "What are the options?\n",
    "\n",
    "(1) 50-50 ---> 492 cases of Fraud and 492 Cases of Non Fraud:\n",
    "<br>a) Since we are are considering all Fraud Cases here, model will overfit resulting in bad outcome when tested against original population as we have consumed entire set of 492 Fraud Cases in training.\n",
    "<br>b) Out of 284k cases we are considering only 492 cases which would result in information loss\n",
    "\n",
    "(2) We can therefore create a downsampled dataset by retaining a stratified 92(~20%)Fraud cases within original dataset as holdout, and create a downsampled data with remaining 400 Fraud Cases together with Non-Fraud cases in 90%-10% Combination (giving more weightage to the Non-Fraud Cases, even in downsampled data).\n",
    "\n",
    "That means\n",
    "<br>df_Fraud:400 Cases & df_NonFraud:3600 cases. All the remaining cases will stay in Validation DataFrame (df_validn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Nice read**\n",
    "<br> https://machinelearningmastery.com/tactics-to-combat-imbalanced-classes-in-your-machine-learning-dataset/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Under Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Steps to prepare training and validation Dataframes**\n",
    "<br>*To avoid test cases creeping into train cases we will have to split the dataframe into Validation and train dataframes separately and then go on with resampling(Downsampling in this case)*\n",
    "- Split the data into 2 parts: df_Fraud(Target class 1) & df_NonFraud(Target class 0)\n",
    "- Shuffle both DataFrames\n",
    "- Pick only 400 Fraud cases into Fraud train DataFrame\n",
    "- Pick first 3600 cases from shuffled NonFraud train DataFrame (90%-10% combination of NonFraud and Fraud cases in downsampled dataset)\n",
    "- Remaining cases will go into Validation dataframes for Fraud and Non Fraud\n",
    "- Concatinate dataframes to form Validation and train dataframes\n",
    "- Shuffling the Combined DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>...</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Class</th>\n",
       "      <th>amount</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.560829</td>\n",
       "      <td>-1.013368</td>\n",
       "      <td>0.222265</td>\n",
       "      <td>-1.431515</td>\n",
       "      <td>-1.388715</td>\n",
       "      <td>-0.770123</td>\n",
       "      <td>-0.985883</td>\n",
       "      <td>-0.167139</td>\n",
       "      <td>-1.673531</td>\n",
       "      <td>1.470053</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.386785</td>\n",
       "      <td>-0.012937</td>\n",
       "      <td>-0.162839</td>\n",
       "      <td>0.424330</td>\n",
       "      <td>-0.171833</td>\n",
       "      <td>0.024103</td>\n",
       "      <td>0.013327</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.167680</td>\n",
       "      <td>-0.619544</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.406853</td>\n",
       "      <td>0.247479</td>\n",
       "      <td>-0.380177</td>\n",
       "      <td>-0.450693</td>\n",
       "      <td>2.069358</td>\n",
       "      <td>3.929895</td>\n",
       "      <td>-0.832316</td>\n",
       "      <td>0.044340</td>\n",
       "      <td>1.603553</td>\n",
       "      <td>-0.720463</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.717517</td>\n",
       "      <td>-0.119550</td>\n",
       "      <td>0.991588</td>\n",
       "      <td>0.626249</td>\n",
       "      <td>0.354788</td>\n",
       "      <td>0.159912</td>\n",
       "      <td>0.240932</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.135401</td>\n",
       "      <td>-0.951362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.724167</td>\n",
       "      <td>1.166519</td>\n",
       "      <td>0.768264</td>\n",
       "      <td>-0.066122</td>\n",
       "      <td>-0.217705</td>\n",
       "      <td>-0.562777</td>\n",
       "      <td>0.140554</td>\n",
       "      <td>0.617073</td>\n",
       "      <td>-0.547850</td>\n",
       "      <td>-0.520817</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.788336</td>\n",
       "      <td>0.021337</td>\n",
       "      <td>-0.100856</td>\n",
       "      <td>-0.176246</td>\n",
       "      <td>0.082772</td>\n",
       "      <td>0.103339</td>\n",
       "      <td>0.015769</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.167959</td>\n",
       "      <td>-0.178891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.911316</td>\n",
       "      <td>0.351888</td>\n",
       "      <td>0.869489</td>\n",
       "      <td>-1.423147</td>\n",
       "      <td>-0.869785</td>\n",
       "      <td>-0.152330</td>\n",
       "      <td>0.954163</td>\n",
       "      <td>-0.159831</td>\n",
       "      <td>-1.587269</td>\n",
       "      <td>0.755971</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.286046</td>\n",
       "      <td>0.201495</td>\n",
       "      <td>0.185376</td>\n",
       "      <td>-0.176639</td>\n",
       "      <td>-0.575359</td>\n",
       "      <td>0.024290</td>\n",
       "      <td>0.050484</td>\n",
       "      <td>0</td>\n",
       "      <td>1.841543</td>\n",
       "      <td>-0.530199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.298067</td>\n",
       "      <td>0.941754</td>\n",
       "      <td>1.180773</td>\n",
       "      <td>0.670404</td>\n",
       "      <td>0.165205</td>\n",
       "      <td>-0.706999</td>\n",
       "      <td>1.091801</td>\n",
       "      <td>-0.187288</td>\n",
       "      <td>-0.491954</td>\n",
       "      <td>-0.529333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.252905</td>\n",
       "      <td>-0.216221</td>\n",
       "      <td>0.411060</td>\n",
       "      <td>0.430908</td>\n",
       "      <td>-0.319319</td>\n",
       "      <td>0.025450</td>\n",
       "      <td>0.022515</td>\n",
       "      <td>0</td>\n",
       "      <td>0.256969</td>\n",
       "      <td>-0.302753</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0  1.560829 -1.013368  0.222265 -1.431515 -1.388715 -0.770123 -0.985883   \n",
       "1  0.406853  0.247479 -0.380177 -0.450693  2.069358  3.929895 -0.832316   \n",
       "2 -0.724167  1.166519  0.768264 -0.066122 -0.217705 -0.562777  0.140554   \n",
       "3 -0.911316  0.351888  0.869489 -1.423147 -0.869785 -0.152330  0.954163   \n",
       "4 -0.298067  0.941754  1.180773  0.670404  0.165205 -0.706999  1.091801   \n",
       "\n",
       "         V8        V9       V10  ...       V22       V23       V24       V25  \\\n",
       "0 -0.167139 -1.673531  1.470053  ... -0.386785 -0.012937 -0.162839  0.424330   \n",
       "1  0.044340  1.603553 -0.720463  ... -0.717517 -0.119550  0.991588  0.626249   \n",
       "2  0.617073 -0.547850 -0.520817  ... -0.788336  0.021337 -0.100856 -0.176246   \n",
       "3 -0.159831 -1.587269  0.755971  ... -0.286046  0.201495  0.185376 -0.176639   \n",
       "4 -0.187288 -0.491954 -0.529333  ...  0.252905 -0.216221  0.411060  0.430908   \n",
       "\n",
       "        V26       V27       V28  Class    amount      time  \n",
       "0 -0.171833  0.024103  0.013327      0 -0.167680 -0.619544  \n",
       "1  0.354788  0.159912  0.240932      0 -0.135401 -0.951362  \n",
       "2  0.082772  0.103339  0.015769      0 -0.167959 -0.178891  \n",
       "3 -0.575359  0.024290  0.050484      0  1.841543 -0.530199  \n",
       "4 -0.319319  0.025450  0.022515      0  0.256969 -0.302753  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lets split the data into 2 parts: df_Fraud & df_NonFraud\n",
    "df_Fraud=df[df.Class==1]\n",
    "df_NonFraud=df[df.Class==0]\n",
    "\n",
    "# Lets Shuffle the Fraud and Non-Fraud DataFrames\n",
    "from sklearn.utils import shuffle\n",
    "df_Fraud = shuffle(df_Fraud)\n",
    "df_Fraud.reset_index(inplace=True, drop=True)\n",
    "df_NonFraud = shuffle(df_NonFraud)\n",
    "df_NonFraud.reset_index(inplace=True, drop=True)\n",
    "\n",
    "# Lets pick 400 Fraud cases in downsampled data together with 3600(in 90%-10% combination) Non-Fraud cases. \n",
    "# This will form the train dataframe and all the remaining values will go into Validation dataframe\n",
    "\n",
    "# Train DataFrames\n",
    "df_Fraud_train=df_Fraud[:400]\n",
    "df_NonFraud_train=df_NonFraud[:3600]\n",
    "\n",
    "#Validation DataFrames\n",
    "df_Fraud_validn=df_Fraud[400:]\n",
    "df_NonFraud_validn=df_NonFraud[3600:]\n",
    "\n",
    "# Combining both Fraud and Non-Fraud DataFrames to Validation and Train DataFrames\n",
    "df_train=pd.concat([df_Fraud_train,df_NonFraud_train])\n",
    "df_validn=pd.concat([df_Fraud_validn,df_NonFraud_validn])\n",
    "\n",
    "#Shuffling the Combined DataFrame\n",
    "df_train = shuffle(df_train)\n",
    "df_train.reset_index(inplace=True, drop=True)\n",
    "df_validn = shuffle(df_validn)\n",
    "df_validn.reset_index(inplace=True, drop=True)\n",
    "\n",
    "# Overview of Combined DataFrame\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'C': 1.0}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = df_train.drop('Class',axis=1)\n",
    "y = df_train['Class']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=101)\n",
    "\n",
    "parameters = {\n",
    "    'C': np.linspace(1, 10, 10)\n",
    "             }\n",
    "logmodel2 = LogisticRegression()\n",
    "clf = GridSearchCV(logmodel2, parameters, cv=5, verbose=5, n_jobs=3)\n",
    "clf.fit(X_train, y_train.ravel())\n",
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1, verbose=5)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logmodel3 = LogisticRegression(C=1,penalty='l2', verbose=5)\n",
    "logmodel3.fit(X_train, y_train.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predictions and Evaluations\n",
    "##### With Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99       710\n",
      "           1       0.98      0.88      0.92        90\n",
      "\n",
      "    accuracy                           0.98       800\n",
      "   macro avg       0.98      0.94      0.96       800\n",
      "weighted avg       0.98      0.98      0.98       800\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = logmodel3.predict(X_test)\n",
    "print(classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### With Validation Data (remaining population)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    280715\n",
      "           1       0.08      0.82      0.14        92\n",
      "\n",
      "    accuracy                           1.00    280807\n",
      "   macro avg       0.54      0.91      0.57    280807\n",
      "weighted avg       1.00      1.00      1.00    280807\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Lets Separate out Target Class from the Population\n",
    "X_validn_Popln = df_validn.drop('Class',axis=1)\n",
    "y_validn_Popln = df_validn['Class']\n",
    "\n",
    "pred_Popln = logmodel3.predict(X_validn_Popln)\n",
    "\n",
    "print(classification_report(y_validn_Popln,pred_Popln))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'criterion': 'entropy',\n",
       " 'max_depth': 7,\n",
       " 'max_features': 'auto',\n",
       " 'n_estimators': 200}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create an instance of the RandomForestClassifier class and fit it to our training data from the previous step.\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfc=RandomForestClassifier(random_state=42)\n",
    "param_grid = { \n",
    "    'n_estimators': [200, 500],\n",
    "    'max_features': ['auto', 'sqrt', 'log2'],\n",
    "    'max_depth' : [4,5,6,7,8],\n",
    "    'criterion' :['gini', 'entropy']\n",
    "}\n",
    "\n",
    "CV_rfc = GridSearchCV(estimator=rfc, param_grid=param_grid, cv= 5)\n",
    "CV_rfc.fit(X_train, y_train)\n",
    "\n",
    "CV_rfc.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(criterion='entropy', max_depth=7, n_estimators=200,\n",
       "                       random_state=42)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CV_rfc1=RandomForestClassifier(random_state=42, max_features='auto', n_estimators= 200, max_depth=7, criterion='entropy')\n",
    "CV_rfc1.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predictions and Evaluations\n",
    "##### Evaluating with Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99       710\n",
      "           1       0.99      0.88      0.93        90\n",
      "\n",
      "    accuracy                           0.98       800\n",
      "   macro avg       0.99      0.94      0.96       800\n",
      "weighted avg       0.99      0.98      0.98       800\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = CV_rfc1.predict(X_test)\n",
    "print(classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluating with Original Population(Validation DataFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    280715\n",
      "           1       0.26      0.80      0.39        92\n",
      "\n",
      "    accuracy                           1.00    280807\n",
      "   macro avg       0.63      0.90      0.69    280807\n",
      "weighted avg       1.00      1.00      1.00    280807\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_Popln = CV_rfc1.predict(X_validn_Popln)\n",
    "print(classification_report(y_validn_Popln,pred_Popln))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Over Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Lets check how does Over sampling perform for our case.\n",
    "- we should perform Over sampling only on df_train DataFrame\n",
    "- df_validn DataFrame should be left untouched to avoid overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will consider only 200 Fraud Cases for our Training set along with equal proportion of Non Fraud Cases exactly representing the Original population(99.8%-0.2%). All the remaining cases will stay with validation DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>V10</th>\n",
       "      <th>...</th>\n",
       "      <th>V22</th>\n",
       "      <th>V23</th>\n",
       "      <th>V24</th>\n",
       "      <th>V25</th>\n",
       "      <th>V26</th>\n",
       "      <th>V27</th>\n",
       "      <th>V28</th>\n",
       "      <th>Class</th>\n",
       "      <th>amount</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.912281</td>\n",
       "      <td>-0.046583</td>\n",
       "      <td>1.432522</td>\n",
       "      <td>-1.346622</td>\n",
       "      <td>-0.827506</td>\n",
       "      <td>1.013003</td>\n",
       "      <td>-2.336704</td>\n",
       "      <td>-3.019579</td>\n",
       "      <td>-0.225768</td>\n",
       "      <td>0.343410</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.480636</td>\n",
       "      <td>-2.094932</td>\n",
       "      <td>-0.348946</td>\n",
       "      <td>0.994553</td>\n",
       "      <td>-0.054214</td>\n",
       "      <td>0.464046</td>\n",
       "      <td>0.138535</td>\n",
       "      <td>0</td>\n",
       "      <td>2.487249</td>\n",
       "      <td>0.766327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.111100</td>\n",
       "      <td>0.529973</td>\n",
       "      <td>-2.855060</td>\n",
       "      <td>0.493696</td>\n",
       "      <td>1.117092</td>\n",
       "      <td>-0.973020</td>\n",
       "      <td>0.274044</td>\n",
       "      <td>-0.243261</td>\n",
       "      <td>0.611078</td>\n",
       "      <td>-1.193171</td>\n",
       "      <td>...</td>\n",
       "      <td>0.255346</td>\n",
       "      <td>-0.099226</td>\n",
       "      <td>-0.195407</td>\n",
       "      <td>0.362483</td>\n",
       "      <td>-0.088946</td>\n",
       "      <td>0.011381</td>\n",
       "      <td>0.006377</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.293440</td>\n",
       "      <td>0.510039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.173508</td>\n",
       "      <td>-0.972200</td>\n",
       "      <td>0.014131</td>\n",
       "      <td>-0.646410</td>\n",
       "      <td>-2.588478</td>\n",
       "      <td>1.011115</td>\n",
       "      <td>1.513677</td>\n",
       "      <td>-2.391082</td>\n",
       "      <td>1.674079</td>\n",
       "      <td>-1.505965</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.705146</td>\n",
       "      <td>-2.615818</td>\n",
       "      <td>0.149687</td>\n",
       "      <td>0.193488</td>\n",
       "      <td>1.270039</td>\n",
       "      <td>0.613645</td>\n",
       "      <td>0.052868</td>\n",
       "      <td>0</td>\n",
       "      <td>13.665898</td>\n",
       "      <td>0.134271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.238839</td>\n",
       "      <td>-0.150034</td>\n",
       "      <td>0.188254</td>\n",
       "      <td>-0.300057</td>\n",
       "      <td>-0.323609</td>\n",
       "      <td>-0.201555</td>\n",
       "      <td>-0.296069</td>\n",
       "      <td>0.141340</td>\n",
       "      <td>0.197968</td>\n",
       "      <td>-0.028027</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.233095</td>\n",
       "      <td>0.039172</td>\n",
       "      <td>-0.269361</td>\n",
       "      <td>0.140477</td>\n",
       "      <td>0.997069</td>\n",
       "      <td>-0.079332</td>\n",
       "      <td>-0.016200</td>\n",
       "      <td>0</td>\n",
       "      <td>-0.297771</td>\n",
       "      <td>-0.022474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-3.352130</td>\n",
       "      <td>-6.519216</td>\n",
       "      <td>0.441358</td>\n",
       "      <td>-1.035620</td>\n",
       "      <td>4.086863</td>\n",
       "      <td>-4.349348</td>\n",
       "      <td>-3.966769</td>\n",
       "      <td>0.214478</td>\n",
       "      <td>0.318892</td>\n",
       "      <td>0.985043</td>\n",
       "      <td>...</td>\n",
       "      <td>0.280237</td>\n",
       "      <td>1.623202</td>\n",
       "      <td>-0.146730</td>\n",
       "      <td>-1.888590</td>\n",
       "      <td>-0.659871</td>\n",
       "      <td>0.143194</td>\n",
       "      <td>0.469116</td>\n",
       "      <td>0</td>\n",
       "      <td>0.922239</td>\n",
       "      <td>0.063969</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         V1        V2        V3        V4        V5        V6        V7  \\\n",
       "0 -0.912281 -0.046583  1.432522 -1.346622 -0.827506  1.013003 -2.336704   \n",
       "1  2.111100  0.529973 -2.855060  0.493696  1.117092 -0.973020  0.274044   \n",
       "2 -1.173508 -0.972200  0.014131 -0.646410 -2.588478  1.011115  1.513677   \n",
       "3  1.238839 -0.150034  0.188254 -0.300057 -0.323609 -0.201555 -0.296069   \n",
       "4 -3.352130 -6.519216  0.441358 -1.035620  4.086863 -4.349348 -3.966769   \n",
       "\n",
       "         V8        V9       V10  ...       V22       V23       V24       V25  \\\n",
       "0 -3.019579 -0.225768  0.343410  ... -1.480636 -2.094932 -0.348946  0.994553   \n",
       "1 -0.243261  0.611078 -1.193171  ...  0.255346 -0.099226 -0.195407  0.362483   \n",
       "2 -2.391082  1.674079 -1.505965  ... -0.705146 -2.615818  0.149687  0.193488   \n",
       "3  0.141340  0.197968 -0.028027  ... -0.233095  0.039172 -0.269361  0.140477   \n",
       "4  0.214478  0.318892  0.985043  ...  0.280237  1.623202 -0.146730 -1.888590   \n",
       "\n",
       "        V26       V27       V28  Class     amount      time  \n",
       "0 -0.054214  0.464046  0.138535      0   2.487249  0.766327  \n",
       "1 -0.088946  0.011381  0.006377      0  -0.293440  0.510039  \n",
       "2  1.270039  0.613645  0.052868      0  13.665898  0.134271  \n",
       "3  0.997069 -0.079332 -0.016200      0  -0.297771 -0.022474  \n",
       "4 -0.659871  0.143194  0.469116      0   0.922239  0.063969  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lets split the data into 2 parts: df_Fraud & df_NonFraud\n",
    "df_Fraud=df[df.Class==1]\n",
    "df_NonFraud=df[df.Class==0]\n",
    "\n",
    "# Lets Shuffle the Fraud and Non-Fraud DataFrames\n",
    "from sklearn.utils import shuffle\n",
    "df_Fraud = shuffle(df_Fraud)\n",
    "df_Fraud.reset_index(inplace=True, drop=True)\n",
    "df_NonFraud = shuffle(df_NonFraud)\n",
    "df_NonFraud.reset_index(inplace=True, drop=True)\n",
    "\n",
    "# Train DataFrames\n",
    "df_Fraud_train=df_Fraud[:200]\n",
    "df_NonFraud_train=df_NonFraud[:round(200/0.002)]\n",
    "\n",
    "#Validation DataFrames\n",
    "df_Fraud_validn=df_Fraud[200:]\n",
    "df_NonFraud_validn=df_NonFraud[round(200/0.002):]\n",
    "\n",
    "# Combining both Fraud and Non-Fraud DataFrames to Validation and Train DataFrames\n",
    "df_train=pd.concat([df_Fraud_train,df_NonFraud_train])\n",
    "df_validn=pd.concat([df_Fraud_validn,df_NonFraud_validn])\n",
    "\n",
    "#Shuffling the Combined DataFrame\n",
    "df_train = shuffle(df_train)\n",
    "df_train.reset_index(inplace=True, drop=True)\n",
    "df_validn = shuffle(df_validn)\n",
    "df_validn.reset_index(inplace=True, drop=True)\n",
    "\n",
    "# Overview of Combined DataFrame\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from imblearn.over_sampling import SMOTE\n",
    "sm = SMOTE(random_state=2)\n",
    "\n",
    "X = df_train.drop('Class',axis=1)\n",
    "y = df_train['Class']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=101)\n",
    "\n",
    "X_train_res, y_train_res = sm.fit_sample(X_train, y_train.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=LogisticRegression(), n_jobs=3,\n",
       "             param_grid={'C': array([ 1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10.])},\n",
       "             verbose=5)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "parameters = {\n",
    "    'C': np.linspace(1, 10, 10)\n",
    "             }\n",
    "lr = LogisticRegression()\n",
    "clf = GridSearchCV(lr, parameters, cv=5, verbose=5, n_jobs=3)\n",
    "clf.fit(X_train_res, y_train_res.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 5.0}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    2.4s remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    2.4s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=5, verbose=5)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr1 = LogisticRegression(C=5,penalty='l2', verbose=5)\n",
    "lr1.fit(X_train_res, y_train_res.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predictions and Evaluations\n",
    "##### Evaluating with Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[19365   637]\n",
      " [    0    38]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.97      0.98     20002\n",
      "           1       0.06      1.00      0.11        38\n",
      "\n",
      "    accuracy                           0.97     20040\n",
      "   macro avg       0.53      0.98      0.55     20040\n",
      "weighted avg       1.00      0.97      0.98     20040\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = lr1.predict(X_test)\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation with Validation Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[178048   6267]\n",
      " [    22    270]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.97      0.98    184315\n",
      "           1       0.04      0.92      0.08       292\n",
      "\n",
      "    accuracy                           0.97    184607\n",
      "   macro avg       0.52      0.95      0.53    184607\n",
      "weighted avg       1.00      0.97      0.98    184607\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Lets Separate out Target Class from the Population\n",
    "X_validn_Popln = df_validn.drop('Class',axis=1)\n",
    "y_validn_Popln = df_validn['Class']\n",
    "\n",
    "pred_Popln = lr1.predict(X_validn_Popln)\n",
    "print(confusion_matrix(y_validn_Popln,pred_Popln))\n",
    "\n",
    "print(classification_report(y_validn_Popln,pred_Popln))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(min_samples_split=10, n_estimators=1000, n_jobs=-1,\n",
       "                       oob_score=True, random_state=1)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create an instance of the RandomForestClassifier class and fit it to our training data.\n",
    "CV_rfc3=RandomForestClassifier(n_estimators=1000, min_samples_split=10, min_samples_leaf=1,\n",
    "           max_features='auto', max_leaf_nodes=None,\n",
    "           oob_score=True, n_jobs=-1, random_state=1)\n",
    "CV_rfc3.fit(X_train_res, y_train_res.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predictions and Evaluations\n",
    "##### Evaluating with Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     20002\n",
      "           1       0.92      0.89      0.91        38\n",
      "\n",
      "    accuracy                           1.00     20040\n",
      "   macro avg       0.96      0.95      0.95     20040\n",
      "weighted avg       1.00      1.00      1.00     20040\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = CV_rfc3.predict(X_test)\n",
    "print(classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluating with Original Population(Validation DataFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    184315\n",
      "           1       0.88      0.83      0.86       292\n",
      "\n",
      "    accuracy                           1.00    184607\n",
      "   macro avg       0.94      0.92      0.93    184607\n",
      "weighted avg       1.00      1.00      1.00    184607\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred_Popln = CV_rfc3.predict(X_validn_Popln)\n",
    "print(classification_report(y_validn_Popln,pred_Popln))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](Table.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The data to start with was highly imbalanced with Target Class = Fraud contributing only 0.2% (492 Cases) where as remaining 99.8% of the data was Non Fraud. \n",
    "- Logistic Regression performed on Without resampling case yields a recall of 61% which means out of all the Fraud cases only 61% are predicted correctly\n",
    "- Under sampling doesn't seem to be working(both with Logistic regression and Random forest) with a maximum precision of 26%. Which means out of all the cases which are predicted to be Fraud only 26% are Actually Fraud. Remaining 74% are the Non-Fraud cases wrongly classified as Frauds.\n",
    "- Logistic Regression in Over sampling doesn't throw encouraging results either however **Random Forest in Over Sampling** (Performed with SMOTE) gives much better result with 88% Precision and 83% of Recall.\n",
    "- We can therefore settle with Random Forests for Over Sampling.\n",
    "- I will explore other methods in future."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
